#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Docker Composeåˆ†æå™¨
åˆ†ædocker-compose.ymlæ–‡ä»¶ï¼Œè¯„ä¼°éƒ¨ç½²å¤æ‚åº¦å’Œç§»æ¤éš¾åº¦
"""

import yaml
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import json

@dataclass
class ServiceInfo:
    """æœåŠ¡ä¿¡æ¯æ•°æ®ç±»"""
    name: str
    image: str
    ports: List[str]
    volumes: List[str]
    environment: Dict[str, str]
    depends_on: List[str]
    networks: List[str]
    restart_policy: str
    build_context: Optional[str] = None

@dataclass
class DockerComposeAnalysis:
    """Docker Composeåˆ†æç»“æœæ•°æ®ç±»"""
    services_count: int
    total_ports: int
    exposed_ports: List[int]
    requires_build: bool
    external_dependencies: List[str]
    storage_requirements: List[str]
    network_requirements: List[str]
    complexity_level: str  # ç®€å•/ä¸­ç­‰/å¤æ‚
    complexity_score: int  # 0-100
    deployment_notes: List[str]
    migration_warnings: List[str]
    services: List[ServiceInfo]

class DockerComposeAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.common_databases = [
            'mysql', 'postgres', 'postgresql', 'mongodb', 'redis', 
            'elasticsearch', 'mariadb', 'influxdb', 'clickhouse'
        ]
        self.common_services = [
            'nginx', 'apache', 'traefik', 'caddy', 'haproxy',
            'rabbitmq', 'kafka', 'prometheus', 'grafana'
        ]
        
    def analyze_docker_compose(self, content: str) -> DockerComposeAnalysis:
        """
        åˆ†ædocker-compose.ymlå†…å®¹
        
        :param content: docker-compose.ymlæ–‡ä»¶å†…å®¹
        :return: åˆ†æç»“æœ
        """
        try:
            # è§£æYAML
            compose_data = yaml.safe_load(content)
            
            if not compose_data or 'services' not in compose_data:
                raise ValueError("æ— æ•ˆçš„docker-compose.ymlæ–‡ä»¶")
            
            services = compose_data['services']
            services_info = []
            
            # åˆ†ææ¯ä¸ªæœåŠ¡
            for service_name, service_config in services.items():
                service_info = self._analyze_service(service_name, service_config)
                services_info.append(service_info)
            
            # ç”Ÿæˆæ•´ä½“åˆ†æ
            return self._generate_analysis(services_info, compose_data)
            
        except yaml.YAMLError as e:
            raise ValueError(f"YAMLè§£æé”™è¯¯: {e}")
    
    def _analyze_service(self, name: str, config: Dict) -> ServiceInfo:
        """åˆ†æå•ä¸ªæœåŠ¡"""
        return ServiceInfo(
            name=name,
            image=config.get('image', ''),
            ports=self._extract_ports(config.get('ports', [])),
            volumes=self._extract_volumes(config.get('volumes', [])),
            environment=self._extract_environment(config.get('environment', {})),
            depends_on=config.get('depends_on', []) if isinstance(config.get('depends_on', []), list) else list(config.get('depends_on', {}).keys()),
            networks=config.get('networks', []) if isinstance(config.get('networks', []), list) else list(config.get('networks', {}).keys()),
            restart_policy=config.get('restart', 'no'),
            build_context=config.get('build', {}).get('context') if isinstance(config.get('build'), dict) else config.get('build')
        )
    
    def _extract_ports(self, ports: List) -> List[str]:
        """æå–ç«¯å£æ˜ å°„"""
        port_list = []
        for port in ports:
            if isinstance(port, str):
                port_list.append(port)
            elif isinstance(port, int):
                port_list.append(str(port))
            elif isinstance(port, dict):
                target = port.get('target', '')
                published = port.get('published', '')
                if target and published:
                    port_list.append(f"{published}:{target}")
        return port_list
    
    def _extract_volumes(self, volumes: List) -> List[str]:
        """æå–å·æ˜ å°„"""
        volume_list = []
        for volume in volumes:
            if isinstance(volume, str):
                volume_list.append(volume)
            elif isinstance(volume, dict):
                source = volume.get('source', '')
                target = volume.get('target', '')
                if source and target:
                    volume_list.append(f"{source}:{target}")
        return volume_list
    
    def _extract_environment(self, env) -> Dict[str, str]:
        """æå–ç¯å¢ƒå˜é‡"""
        if isinstance(env, list):
            env_dict = {}
            for item in env:
                if '=' in str(item):
                    key, value = str(item).split('=', 1)
                    env_dict[key] = value
            return env_dict
        elif isinstance(env, dict):
            return {k: str(v) for k, v in env.items()}
        return {}
    
    def _generate_analysis(self, services_info: List[ServiceInfo], compose_data: Dict) -> DockerComposeAnalysis:
        """ç”Ÿæˆåˆ†æç»“æœ"""
        
        # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
        services_count = len(services_info)
        all_ports = []
        requires_build = False
        external_deps = []
        storage_reqs = []
        network_reqs = []
        deployment_notes = []
        migration_warnings = []
        
        # åˆ†ææ¯ä¸ªæœåŠ¡
        for service in services_info:
            # æ”¶é›†ç«¯å£
            for port in service.ports:
                if ':' in port:
                    external_port = port.split(':')[0]
                    try:
                        all_ports.append(int(external_port))
                    except ValueError:
                        pass
                else:
                    try:
                        all_ports.append(int(port))
                    except ValueError:
                        pass
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ„å»º
            if service.build_context:
                requires_build = True
                deployment_notes.append(f"æœåŠ¡ '{service.name}' éœ€è¦æœ¬åœ°æ„å»º")
            
            # æ£€æŸ¥å¤–éƒ¨ä¾èµ–
            image = service.image.lower()
            for db in self.common_databases:
                if db in image:
                    external_deps.append(f"æ•°æ®åº“: {db}")
                    break
            
            for svc in self.common_services:
                if svc in image:
                    external_deps.append(f"æœåŠ¡: {svc}")
                    break
            
            # å­˜å‚¨éœ€æ±‚
            for volume in service.volumes:
                if not volume.startswith('/tmp') and ':' in volume:
                    host_path = volume.split(':')[0]
                    if not host_path.startswith('.') and not host_path.startswith('/'):
                        storage_reqs.append(f"æŒä¹…åŒ–å­˜å‚¨: {volume}")
                    else:
                        storage_reqs.append(f"æœ¬åœ°å­˜å‚¨: {volume}")
        
        # ç½‘ç»œéœ€æ±‚
        if 'networks' in compose_data:
            for network_name, network_config in compose_data['networks'].items():
                if isinstance(network_config, dict) and network_config.get('external'):
                    network_reqs.append(f"å¤–éƒ¨ç½‘ç»œ: {network_name}")
                else:
                    network_reqs.append(f"å†…éƒ¨ç½‘ç»œ: {network_name}")
        
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        complexity_score = self._calculate_complexity_score(
            services_count, len(all_ports), requires_build, 
            len(external_deps), len(storage_reqs), len(network_reqs)
        )
        
        # ç¡®å®šå¤æ‚åº¦ç­‰çº§
        if complexity_score <= 30:
            complexity_level = "ç®€å•"
        elif complexity_score <= 60:
            complexity_level = "ä¸­ç­‰"
        else:
            complexity_level = "å¤æ‚"
        
        # ç”Ÿæˆéƒ¨ç½²æ³¨æ„äº‹é¡¹
        if services_count > 5:
            deployment_notes.append("æœåŠ¡æ•°é‡è¾ƒå¤šï¼Œå»ºè®®åˆ†é˜¶æ®µéƒ¨ç½²")
        
        if len(all_ports) > 10:
            migration_warnings.append("ç«¯å£æ•°é‡è¾ƒå¤šï¼Œéœ€è¦æ£€æŸ¥ç«¯å£å†²çª")
        
        if requires_build:
            migration_warnings.append("éœ€è¦æœ¬åœ°æ„å»ºé•œåƒï¼Œç¡®ä¿æ„å»ºç¯å¢ƒæ­£ç¡®")
        
        if external_deps:
            deployment_notes.append("ä¾èµ–å¤–éƒ¨æœåŠ¡ï¼Œéœ€è¦é¢„å…ˆå‡†å¤‡")
        
        # å»é‡
        external_deps = list(set(external_deps))
        storage_reqs = list(set(storage_reqs))
        network_reqs = list(set(network_reqs))
        
        return DockerComposeAnalysis(
            services_count=services_count,
            total_ports=len(all_ports),
            exposed_ports=sorted(set(all_ports)),
            requires_build=requires_build,
            external_dependencies=external_deps,
            storage_requirements=storage_reqs,
            network_requirements=network_reqs,
            complexity_level=complexity_level,
            complexity_score=complexity_score,
            deployment_notes=deployment_notes,
            migration_warnings=migration_warnings,
            services=services_info
        )
    
    def _calculate_complexity_score(self, services_count: int, ports_count: int, 
                                  requires_build: bool, deps_count: int,
                                  storage_count: int, network_count: int) -> int:
        """
        è®¡ç®—å¤æ‚åº¦åˆ†æ•° (0-100)
        """
        score = 0
        
        # æœåŠ¡æ•°é‡æƒé‡ (0-30åˆ†)
        score += min(services_count * 5, 30)
        
        # ç«¯å£æ•°é‡æƒé‡ (0-15åˆ†)
        score += min(ports_count * 2, 15)
        
        # æ„å»ºéœ€æ±‚æƒé‡ (0-20åˆ†)
        if requires_build:
            score += 20
        
        # å¤–éƒ¨ä¾èµ–æƒé‡ (0-20åˆ†)
        score += min(deps_count * 5, 20)
        
        # å­˜å‚¨éœ€æ±‚æƒé‡ (0-10åˆ†)
        score += min(storage_count * 2, 10)
        
        # ç½‘ç»œéœ€æ±‚æƒé‡ (0-5åˆ†)
        score += min(network_count * 1, 5)
        
        return min(score, 100)
    
    def generate_deployment_preview(self, analysis: DockerComposeAnalysis) -> str:
        """
        ç”Ÿæˆéƒ¨ç½²é¢„è§ˆæ–‡æ¡£
        
        :param analysis: åˆ†æç»“æœ
        :return: éƒ¨ç½²é¢„è§ˆæ–‡æœ¬
        """
        lines = []
        lines.append("# Docker Compose éƒ¨ç½²åˆ†ææŠ¥å‘Š")
        lines.append("=" * 50)
        lines.append("")
        
        # åŸºæœ¬ä¿¡æ¯
        lines.append("## åŸºæœ¬ä¿¡æ¯")
        lines.append(f"- **æœåŠ¡æ•°é‡**: {analysis.services_count}")
        lines.append(f"- **å¤æ‚åº¦ç­‰çº§**: {analysis.complexity_level} ({analysis.complexity_score}/100)")
        lines.append(f"- **ç«¯å£éœ€æ±‚**: {analysis.total_ports} ä¸ªç«¯å£")
        lines.append(f"- **éœ€è¦æ„å»º**: {'æ˜¯' if analysis.requires_build else 'å¦'}")
        lines.append("")
        
        # ç«¯å£æ˜ å°„
        if analysis.exposed_ports:
            lines.append("## ç«¯å£æ˜ å°„")
            for port in analysis.exposed_ports:
                lines.append(f"- `{port}`")
            lines.append("")
        
        # å¤–éƒ¨ä¾èµ–
        if analysis.external_dependencies:
            lines.append("## å¤–éƒ¨ä¾èµ–")
            for dep in analysis.external_dependencies:
                lines.append(f"- {dep}")
            lines.append("")
        
        # å­˜å‚¨éœ€æ±‚
        if analysis.storage_requirements:
            lines.append("## å­˜å‚¨éœ€æ±‚")
            for storage in analysis.storage_requirements:
                lines.append(f"- {storage}")
            lines.append("")
        
        # ç½‘ç»œéœ€æ±‚
        if analysis.network_requirements:
            lines.append("## ç½‘ç»œéœ€æ±‚")
            for network in analysis.network_requirements:
                lines.append(f"- {network}")
            lines.append("")
        
        # éƒ¨ç½²æ³¨æ„äº‹é¡¹
        if analysis.deployment_notes:
            lines.append("## éƒ¨ç½²æ³¨æ„äº‹é¡¹")
            for note in analysis.deployment_notes:
                lines.append(f"- âš ï¸  {note}")
            lines.append("")
        
        # è¿ç§»è­¦å‘Š
        if analysis.migration_warnings:
            lines.append("## è¿ç§»è­¦å‘Š")
            for warning in analysis.migration_warnings:
                lines.append(f"- âŒ {warning}")
            lines.append("")
        
        # æœåŠ¡è¯¦æƒ…
        lines.append("## æœåŠ¡è¯¦æƒ…")
        for service in analysis.services:
            lines.append(f"### {service.name}")
            lines.append(f"- **é•œåƒ**: `{service.image}`")
            if service.ports:
                lines.append(f"- **ç«¯å£**: {', '.join(service.ports)}")
            if service.volumes:
                lines.append(f"- **å·**: {len(service.volumes)} ä¸ª")
            if service.environment:
                lines.append(f"- **ç¯å¢ƒå˜é‡**: {len(service.environment)} ä¸ª")
            if service.depends_on:
                lines.append(f"- **ä¾èµ–æœåŠ¡**: {', '.join(service.depends_on)}")
            lines.append("")
        
        return "\n".join(lines)
    
    def save_analysis(self, analysis: DockerComposeAnalysis, filepath: str):
        """
        ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
        
        :param analysis: åˆ†æç»“æœ
        :param filepath: æ–‡ä»¶è·¯å¾„
        """
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        analysis_dict = asdict(analysis)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(analysis_dict, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")

def main():
    """æµ‹è¯•å‡½æ•°"""
    print("ğŸ” Docker Compose åˆ†æå™¨æµ‹è¯•")
    print("=" * 50)
    
    # ç¤ºä¾‹docker-compose.ymlå†…å®¹
    sample_compose = """
version: '3.8'

services:
  web:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/ssl
    depends_on:
      - app
    restart: unless-stopped

  app:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/mydb
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./data:/app/data
    depends_on:
      - db
      - redis
    restart: unless-stopped

  db:
    image: postgres:13
    environment:
      POSTGRES_DB: mydb
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    volumes:
      - db_data:/var/lib/postgresql/data
    restart: unless-stopped

  redis:
    image: redis:6-alpine
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  db_data:
  redis_data:

networks:
  default:
    driver: bridge
"""
    
    # åˆ›å»ºåˆ†æå™¨å¹¶åˆ†æ
    analyzer = DockerComposeAnalyzer()
    
    try:
        analysis = analyzer.analyze_docker_compose(sample_compose)
        
        print(f"âœ… åˆ†æå®Œæˆ!")
        print(f"æœåŠ¡æ•°é‡: {analysis.services_count}")
        print(f"å¤æ‚åº¦: {analysis.complexity_level} ({analysis.complexity_score}/100)")
        print(f"ç«¯å£æ•°é‡: {analysis.total_ports}")
        print(f"éœ€è¦æ„å»º: {analysis.requires_build}")
        print()
        
        # ç”Ÿæˆé¢„è§ˆ
        preview = analyzer.generate_deployment_preview(analysis)
        print("ğŸ“‹ éƒ¨ç½²é¢„è§ˆ:")
        print(preview[:500] + "..." if len(preview) > 500 else preview)
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

if __name__ == '__main__':
    main()